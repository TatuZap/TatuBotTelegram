import os
import json
import pickle
from xml.dom.expatbuilder import InternalSubsetExtractor
# from matplotlib.pyplot import get
import nltk
import random
import pandas as pd
import numpy as np
import warnings
import re
import string
import unidecode
from dataclasses import dataclass
from typing import Callable

import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk import word_tokenize, pos_tag
from nltk.stem import WordNetLemmatizer
from enelvo.normaliser import Normaliser
from tensorflow.keras.utils import pad_sequences
from keras.preprocessing.text import Tokenizer



from datetime import datetime,timedelta

# deixe a linha abaixo sem comentÃ¡rios somente se precisar dessas bibliotecas de nlp

@dataclass
class MessageUtils:
    """
        Classe que concentra todo o processamento necessÃ¡rio para
        textos de forma numÃ©rica.
    """
    url_pattern = 'http\S+|www\S+|https\S+'
    punctuacion_pattern = r'[^\w\s]'
    multiple_backspace_pattern = r'\s+'
    emote_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F1E0-\U0001F1FF"
            u"\U00002500-\U00002BEF"
            u"\U00002702-\U000027B0"
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            u"\U0001f926-\U0001f937"
            u"\U00010000-\U0010ffff"
            u"\u2640-\u2642"
            u"\u2600-\u2B55"
            u"\u200d"
            u"\u23cf"
            u"\u23e9"
            u"\u231a"
            u"\ufe0f"
            u"\u3030"
            "]+",re.UNICODE)

    normalizer =  Normaliser(tokenizer='readable')

    clean_by_sub  = lambda self, message, pattern, sub_string : re.sub(pattern,sub_string,message)

    clean_emotes = lambda self, message, : self.clean_by_sub(message, self.emote_pattern, '')
    clean_urls   = lambda self, message, : self.clean_by_sub(message, self.url_pattern, '')
    clean_punctuation = lambda self, message :  self.clean_by_sub(message, self.punctuacion_pattern, '')
    clean_multiple_backspaces = lambda self, message : self.clean_by_sub(message, self.multiple_backspace_pattern, ' ')

    normalize_text = lambda self, message : self.normalizer.normalise(message)

    def full_clean_text(self,message): 
        #message = self.clean_emotes(message.lower())
        message = self.normalize_text(message)
        #message = self.clean_urls(message)
        message = self.clean_punctuation(message)
        message = self.clean_multiple_backspaces(message)
        message = unidecode.unidecode(message)
        return message

    def get_tokens(self,message, tokenizer = None, stopwords=nltk.corpus.stopwords.words('portuguese')):
        return [ word for word in nltk.word_tokenize(self.full_clean_text(message)) if word not in stopwords]


    def bag_of_words_corpus_rep(self):
        """
            FunÃ§Ã£o que devolve a representaÃ§Ã£o bag of words para
            um determinado corpus de entrada.
        """
        bag_X_and_Y = []
        output_empty = [0] * len(self.classes)
        for document in self.documents:
            bag = []
            pattern_words = document[0]
            for word in self.vocabulary:
                bag.append(1) if word in pattern_words else bag.append(0)

            # output_row atuarÃ¡ como uma chave para a lista, 
            # onde a saida serÃ¡ 0 para cada tag e 1 para a tag atual
            output_row = list(output_empty)
            output_row[self.classes.index(document[1])] = 1

            bag_X_and_Y.append([bag, output_row])

        random.shuffle(bag_X_and_Y)
        bag_X_and_Y = np.array(bag_X_and_Y)
        # criamos lista de treino sendo x os patterns e y as intenÃ§Ãµes
        self.X = np.array(list(bag_X_and_Y[:, 0]))
        self.Y = np.array(list(bag_X_and_Y[:, 1]))



    def process_training_data(self, corpus, stopwords=nltk.corpus.stopwords.words('portuguese')):
        self.corpus = corpus
        self.vocabulary = []
        self.documents  = []
        intents    = corpus

        self.classes = [ intent['tag'] for intent in intents['intents']]

        for intent in intents['intents']:
            for pattern in intent['patterns']:
                word = self.get_tokens(pattern)
                self.vocabulary.extend(word)
                self.documents.append((word, intent['tag']))

        self.vocabulary = sorted(list(set(self.vocabulary)))
        self.classes = sorted(list(set(self.classes)))

        pickle.dump(self.vocabulary, open('train_vocabulary.pkl', 'wb'))
        pickle.dump(self.classes, open('train_classes.pkl', 'wb'))
        self.bag_of_words_corpus_rep()


    def bag_for_message(self, message):
        sentence_words = self.get_tokens(message)

        bag = [0]*len(self.vocabulary)
        for setence in sentence_words:
            for i, word in enumerate(self.vocabulary):
                if word == setence:
                    bag[i] = 1
        return(np.array(bag))

    def show_data(self):
        return pd.DataFrame(self.documents, columns = ['Message_tokens', 'Intent'])

    def preprocess_lstm(self):
        df = pd.DataFrame(self.message_utils.documents,columns = ["token-frase","classe"])
        df["texto-lstm"] = df["token-frase"].apply( lambda message : " ".join(message))
        df = df.drop("token-frase",axis="columns")
        MAX_LEN   = len(self.message_utils.vocabulary)
        tokenizer = Tokenizer(MAX_LEN,lower=True)
        tokenizer.fit_on_texts(df['texto-lstm'].values)
        X = tokenizer.texts_to_sequences(df['texto-lstm'].values)
        self.X = pad_sequences(X, maxlen=MAX_LEN)
        self.Y = pd.get_dummies(df['classe']).values



    def __valid_path__(self):
        return os.path.exists(self.dfa_file)

    def load_data(self):
        if self.__valid_path__(self):
            return json.load(self.dfa_file)

    def is_ra(self,message):
        possible_ra = re.findall('\d+', message)
        lenght_constraint = lambda x : len(x) == 8 or len(x) == 11
        valid_ra = list(filter(lenght_constraint,possible_ra))
        return None if valid_ra == [] else valid_ra[0]


    '''
        check_origin: dada uma mensagem, determina qual a origem (sa ou sbc) e qual a origem (sa ou sbc)
    '''

    def check_origin(self, message):
        lista = [] # Origem, Destino, Horario atual ,Horario atual + 1 hora (limite), Dia da semana (0-6)

        origem = 'SA' if re.findall(r'(de.)(sa\b|sta|santo andre)', message) else 'SA' if re.findall('(sa|sta|santo andre).(para|pra)', message) else 'TERMINAL-SBC' if re.findall(r'(terminal|estacao).(sbc|sao bernardo).(para|pra)',message) else 'SBC' if re.findall(r'de.(sbc|sao bernardo).', message) else 'SBC' if re.findall(r'(sbc|sao bernardo).(para|pra).', message) else None
        now = datetime.now()
        time20h = now.replace(hour=20, minute=0, second=0, microsecond=0)
        if now > time20h:
            destino = 'SA' if re.findall(r'(para|pra).(sa\b|sta|santo andre)', message) else 'TERMINAL-SBC' if re.findall(r'(para|pra).(terminal|estacao).(sbc|sao bernardo)',message) else 'SBC' if re.findall(r'(para|pra).(sbc|sao bernardo)', message) else 'SBC' if re.findall(r'(para|pra|pro).(terminal|terminal leste|terminal celso daniel|estaÃ§Ã£o)', message) else None
        else:
            destino = 'SA' if re.findall(r'(para|pra).(sa\b|sta|santo andre)', message) else 'TERMINAL-SBC' if re.findall(r'(para|pra).(terminal|estacao).(sbc|sao bernardo)',message) else 'SBC' if re.findall(r'(para|pra).(sbc|sao bernardo)', message) else None


        if origem and destino :
            now = datetime.now()
            limite = now  + timedelta(hours=2)
            current_time = now.strftime("%H:%M")
            limite_time = limite.strftime("%H:%M")
            lista.append(origem)
            lista.append(destino)
            lista.append(current_time)
            lista.append(limite_time)
            lista.append(datetime.now().weekday())

        return None if lista == [] else lista

def main():
    database = {
        "intents": [
                {
                    "tag": "welcome",
                    "patterns": ["Oi","Oi, bom dia","Oi, boa tarde", "bom dia", "boa tarde", "boa noite", "oi, boa noite", "olÃ¡, boa noite", "oiiiii", "OlÃ¡","oiii, como vai?","opa, tudo bem?"],
                    "responses": ["OlÃ¡, serei seu assistente virtual, em que posso te ajudar?","Salve, qual foi ?", "Manda pro pai, LanÃ§a a braba", "No que posso te ajudar ?"],
                    "context": [""]
                },
                {
                    "tag": "my_classes",
                    "patterns": ["Quais sÃ£o as minhas matÃ©rias ?","Quais sÃ£o as minhas matÃ©rias de hoje ? ","Quais sÃ£o as minhas disciplinas de hoje ? ", "Que aulas eu tenho Hoje","me fale minhas turmas", "que sala eu devo ir?", "Qual minha Sala ?","quais as minhas turmas ?"],
                    "responses": ["Entendi, vocÃª deseja saber suas salas","VocÃª deseja saber suas salas ?", "Ah, vocÃª quer saber qual sala ? ", "Suas Aulas ?"],
                    "context": [""]
                },
                {
                    "tag": "anything_else",
                    "patterns": [],
                    "responses": ["Desculpa, nÃ£o entendi o que vocÃª falou, tente novamente!","NÃ£o compreendi a sua solicitaÃ§Ã£o, talvez eu possa te ajudar"],
                    "context": [""]
                }
            ]
        }
    # demo da funcionalide da classe utils para mensagem
    text_utils = MessageUtils()
    print("Exemplo de pre-procesamento ",text_utils.get_tokens("200 comentÃ¡rios com o emoji do #TimeDoBigode, bora? ğŸ‘¨ğŸ»ğŸ‘¨ğŸ»ğŸ‘¨ğŸ»"))
    text_utils.process_training_data(database,None)
    print("Exemplo de Bag of word ",text_utils.bag_for_message("oi, como vai vocÃª, quais sÃ£o as minhas matÃ©rias de hoje"))
    print(text_utils.vocabulary)
    #print( text_utils.show_data() )
    print( [ text_utils.is_ra(ra) for ra in ["11201722051 mais um ra 11111209","11201721679","11111209","12345"]] )
if __name__ == "__main__":
    main()
